'''
scraper for scraping preprints from psyarxiv 
and posting the most downloaded to mastodon

uses the conda environment in webscrape.yml

PAX API https://developer.osf.io/#tag/General-Usage

Written in a hurry by Tom and chatGPT
'''



# -------------------------------------------set up environment

# libraries we need
import socket #to get host machine identity
import os #file and folder functions

import pandas as pd
from datetime import date,datetime, timedelta
import time
import argparse #for passing command line arguments
import matplotlib.pyplot as plt #for making plots

#selenium stuff for screenshot and getting views
from selenium import webdriver 
from selenium.webdriver.common.by import By 
from selenium.webdriver.firefox.options import Options 
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

#APIs
import requests
from mastodon import Mastodon


#-- main type functions

# look back and count uploads per day, toot the most downloaded
def do_week_review(today,WINDOW):
    '''take todays date and a look back WINDOW days, 
    post screenshot to mastodon of most downloaded''' 
    #calculate WINDOW days ago
    #start_date = datetime.strptime(today, '%Y-%m-%d') - timedelta(days=WINDOW)
    start_date = today - timedelta(days=WINDOW)

    # -------------------------------- SCRAPE
    # Fetch the preprints data
    preprints_data = get_preprints(start_date, today)

    if "data" in preprints_data:
        preprints_df = pd.DataFrame([
            {
                "id": preprint["id"],
                "title": preprint["attributes"]["title"],
                "date_created": preprint["attributes"]["date_created"],
            }
            for preprint in preprints_data["data"]
        ])

        print(str(len(preprints_df)) + " preprints found")

        post_stats(preprints_df)

        # Fetch views and downloads for each preprint
        stats = []
        for preprint_id in preprints_df["id"]:
            if False:
                #screenshot method - takes longer, but gets views
                stat = get_preprint_stats(preprint_id)
            else:
                try:
                    preprint = get_preprint_by_id(preprint_id)
                    downloads = preprint['primary_file']['attributes']['extra']['downloads']
                    stat = {'views': -1, 'downloads': downloads}
                except:
                    stat = {'views': -1, 'downloads': -1}
            stats.append(stat)

        stats_df = pd.DataFrame(stats)
        preprints_df = pd.concat([preprints_df, stats_df], axis=1)

        print(preprints_df)
    else:
        print("Error:", preprints_data.get("error"))    

    preprints_df['downloads'] = pd.to_numeric(preprints_df['downloads'])
    preprints_df.sort_values("downloads",ascending=False,inplace=True)

    #get id and title of mosty downloaded preprint
    most_downloaded = preprints_df.head(1)
    print(most_downloaded)

    preprint_id= most_downloaded['id'].values[0]

    #screenshot method - takes longer, but gets views
    print("getting stats and screenshot for most downloaded preprint")
    stat = get_screenshot(preprint_id)

    
    #-----------------------------POST ---------------------

    #define text 
    uploads = str(len(preprints_df))
    start = start_date.strftime('%Y-%m-%d')
    if 'error' not in stat: 
        views = stat['views'] #str(most_downloaded['views'].values[0])
    else:
        views = -1
    downloads = str(int(most_downloaded['downloads'].values[0]))
    target_url = f"https://osf.io/preprints/psyarxiv/{preprint_id}"

    tootline = "Most downloaded preprint from " + uploads + " uploads since this time on "+ start 
    tootline = tootline + " is \n\n -> " + most_downloaded['title']
    tootline = tootline + "\n\n" + target_url
    tootline = tootline + "\n\nViews: " + views + "\nDownloads: " + downloads
    tootline = tootline + "\n\n(but there's more to life than clicks you know)"
    tootline = tootline + "\n\n#PsyArxiv #Preprints #Psychology"
   
    # ------------------------------------post to mastodon
    alttxt = "Screenshot of psyarxiv page for this preprint. Autogenerated"
    mastodon_post(tootline,mediafile="most.png",alttxt=alttxt)

# make a graph of uploads per day for the previous month, toot that
def do_month_review(today,savename='month.png'):
    #get uploads for each day of month * * before * * current date
    # make graph, post to mastodon
    
    #TAKES ABOUT 7 minutes

    first_day_this_month = today.replace(day=1)  # First day of the current month
    last_day_previous_month = first_day_this_month - timedelta(days=1)  # Last day of the previous month
    first_day_previous_month = last_day_previous_month.replace(day=1)  # First day of the previous month

    start_date=first_day_previous_month
    end_date=last_day_previous_month

    # -------------------------------- SCRAPE
    # Fetch the preprints data
    preprints_data = get_preprints(start_date, end_date)

    #make a df
    if "data" in preprints_data:
        preprints_df = pd.DataFrame([
            {
                "id": preprint["id"],
                "title": preprint["attributes"]["title"],
                "date_created": preprint["attributes"]["date_created"],
            }
            for preprint in preprints_data["data"]
        ])

    print(str(len(preprints_df)) + " preprints found")

    if len(preprints_df) > 0:
        #we're going to group by day of upload
        preprints_df['day'] = preprints_df['date_created'].apply(lambda x:x[:10])

        #count uploads per day
        gb = preprints_df.groupby('day').count()

        monthname = start_date.strftime('%B').upper()

        year = str(start_date.year)

        #make plot
        month_plot(gb,monthname,year,savename)

        #-----------------------------POST ---------------------

        #define text 
        uploads = str(len(preprints_df))
            
        tootline = "PsyArXiv report for " + monthname + " " + year + " \n\n"

        tootline = tootline + uploads + " uploads!\n\n"  

        tootline = tootline + "\n\n" 
        tootline = tootline + "\n\n#PsyArXiv #Preprints #Psychology"
    
        # ------------------------------------post to mastodon
        alttxt = "Bar graph of uploads for " + monthname + ". Autogenerated"
        mastodon_post(tootline,mediafile=savename,alttxt=alttxt)
    else:
        print("No uploads found in this month")

    return len(preprints_df)
  
# get all paper titles and ids for a set period
def do_period_review(start_date, end_date):
    '''save csv of all preprints uploaded between start_date and end_date'''
    # days -> str
    days = str((end_date - start_date).days) 
    #record starting time
    start_time = datetime.now()

    try:
        print("Trying doing period review for " + days + " days")
    
        # -------------------------------- SCRAPE
        # Fetch the preprints data
        preprints_data = get_preprints(start_date, end_date)

        #make a df
        if "data" in preprints_data:
            preprints_df = pd.DataFrame([
                {
                    "id": preprint["id"],
                    "title": preprint["attributes"]["title"],
                    "date_created": preprint["attributes"]["date_created"],
                }
                for preprint in preprints_data["data"]
            ])

        print(str(len(preprints_df)) + " preprints found")

        #we're going to group by day of upload
        preprints_df['day'] = preprints_df['date_created'].apply(lambda x:x[:10])

        #count uploads per day
        gb = preprints_df.groupby('day').count()

        filename = 'data/count'+start_date.isoformat()+ 'to' + end_date.isoformat() + '.csv'
        print("saving to " + filename)
        gb.to_csv(filename)
        filename = 'data/papers'+start_date.isoformat()+ 'to' + end_date.isoformat() + '.csv'
        preprints_df.to_csv(filename)

        end_time = datetime.now()
        time = end_time - start_time
        minutes = str(round(time.total_seconds() / 60,1))
        print("Got data for " + days + " days in " + minutes + " minutes")

    except Exception as e: 
        print(e)
        return None



#-- core function
# called by week, month and period review
def get_preprints(start_date, end_date, filter_type='date_created'):
    # return with list of preprint ids from a time window
    end_date=end_date+timedelta(days=1) #bump 1 to make inclusive
    date_field = filter_type
    api_token = get_PAXtoken() #you may not need to specify this for all queries, but if you have OSF account why not

    # Headers with the API token for authentication
    headers = {
        "Authorization": f"Bearer {api_token}",
    }

    try:
        #preprints_with_contributors = []
        preprints_with_date = []
        next_url = 'https://api.osf.io/v2/preprints/'

        # Convert start_date to a datetime object if it's not already
        #start_date = datetime.fromisoformat(start_date) if isinstance(start_date, str) else start_date

        while next_url:
            # Use `params` only for the initial request
            if "?" not in next_url:  # If no query parameters exist in the URL
                params = {
                    f"filter[{date_field}][gte]": start_date.isoformat(),
                    f"filter[{date_field}][lte]": end_date.isoformat(),
                    "page[size]": 100,  # Max allowed per OSF API documentation
                    "sort": f"-{date_field}",
                    "filter[provider]": "psyarxiv",
                }
            else:
                params = None  # Don't add params again for subsequent requests

            response = requests.get(next_url, headers=headers,params=params)
            response.raise_for_status()

            # Extract preprints and the next page URL
            response_data = response.json()
            preprints = response_data.get('data', [])
            next_url = response_data.get('links', {}).get('next', None)

            for preprint in preprints:
                preprint_date_str = preprint['attributes'][date_field]
                preprint_date = datetime.fromisoformat(preprint_date_str)  # Convert to datetime
                if preprint_date.date() < start_date:
                    # Stop if we encounter a preprint before the start_date
                    next_url = None
                    break
                
                preprints_with_date.append(preprint)

                #add pause to check api timeout
                time.sleep(1)



        #return {"data": preprints_with_contributors}
        return {"data": preprints_with_date}

    except requests.RequestException as e:
        print('Error in get_preprints:', str(e))
        return {"error": str(e)}


#-- helper functions

# post stats as part of week review
def post_stats(preprints_df):
    #toot stats on number of uploads

    #we're going to group by day of upload
    preprints_df['day'] = preprints_df['date_created'].apply(lambda x:x[:10])

    #count uploads per day
    gb = preprints_df.groupby('day').count()
    
    #add a zero count for today's date if it is not in the list
    #get today's date
    today = datetime.now().strftime('%Y-%m-%d')
    
    #add a zero count if today is not in list of dates
    if today not in preprints_df['day'].values:
        gb.loc[today] = [0, 0, 0]
        

    tootline = "STATS! \n\n"
    tootline = tootline + str(gb[1:]['id'].sum()) + " uploads in the last 7 days\n\n"  
    
    for index, row in gb[1:].iterrows():
        tootline = tootline + "\n" + index + " : " + str(row['id']) + " preprints uploaded"
        
    tootline = tootline + ", so far"

    tootline = tootline + "\n\n#PsyArxiv #Preprints #Psychology"
    
    mastodon_post(tootline)

#short way to get downloads per preprint
def get_preprint_by_id(preprint_id):
    ''' get preprint info by id (includes downloads)'''
    try:
        response = requests.get(f'https://api.osf.io/v2/preprints/{preprint_id}/')
        response.raise_for_status()
        preprint = response.json().get('data')

        contributors_url = preprint['relationships']['contributors']['links']['related']['href']
        contributors_response = requests.get(contributors_url)
        contributors_response.raise_for_status()
        preprint['contributors'] = contributors_response.json().get('data', [])

        primary_file_url = preprint['relationships']['primary_file']['links']['related']['href']
        primary_file_response = requests.get(primary_file_url)
        primary_file_response.raise_for_status()
        preprint['primary_file'] = primary_file_response.json().get('data')

        return preprint

    except requests.RequestException as e:
        print('Error in get_preprint_by_id:', str(e))
        return {"error": str(e)}

def get_screenshot(preprint_id):
    ''' take a screenshot of the psyarxiv page for a given preprint'''
    '''also return views and downloads (no vieww count via api?)'''
    
    options = Options()
    options.add_argument("-headless")  # Run in headless mode

    # This allows the page to load for a few seconds before scraping
    wait_time_seconds = 18  # Definitely over 3, over 5 for full safety

    target_url = f"https://osf.io/preprints/psyarxiv/{preprint_id}"

    try:
        driver = webdriver.Firefox(options=options)

        # Open the target website in the browser
        driver.get(target_url)

        # Handle the cookie pop-up
        try:
            cookie_button = WebDriverWait(driver, wait_time_seconds).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(@class, '_Button_6kisxq') and text()='Accept']"))
            )
            cookie_button.click()
            print("Cookie pop-up dismissed.")
        except Exception as e:
            print(f"No cookie pop-up found or it failed to load: {e}")

        element = WebDriverWait(driver, wait_time_seconds).until(
            EC.presence_of_element_located((By.XPATH, "//span[@data-test-view-count]"))
        )
        views = element.text.strip()

        element = WebDriverWait(driver, wait_time_seconds).until(
            EC.presence_of_element_located((By.XPATH, "//span[@data-test-download-count]"))
        )
        downloads = element.text.strip()

        #  Wait for the PDF iframe to load
        pdf_iframe = WebDriverWait(driver, wait_time_seconds).until(
            EC.presence_of_element_located((By.TAG_NAME, "iframe"))
        )
        driver.switch_to.frame(pdf_iframe)
        print("Switched to PDF iframe.")

        # Add a fixed wait time for the iframe to load
        print(f"Waiting {wait_time_seconds} seconds for the PDF to load...")
        time.sleep(wait_time_seconds)
 
        # Save screenshot
        driver.save_screenshot('most.png')
        print("We are in :" + os.getcwd())
        print("Screenshot saved as 'most.png'.")
        
        # Close the browser
        driver.quit()

        return {"views": views, "downloads": downloads}

    except Exception as e:
        print(f"Error occurred: {e}")
        return {"error": "Timeout or another error occurred"}
        
def month_plot(gb,monthname,year,savename='month.png'):
    #make plot of day by day uploads
    print("making plot")

    PAXred = "#cf3d37" #gimp
    PAXblu = "#083050" 

    #calculate average uploads per day
    avg = gb['id'].mean()

    plt.clf()
    plt.bar(gb.index,gb['id'],facecolor=PAXred,edgecolor=PAXblu)
    #horizontal line for average
    plt.axhline(y=avg,color=PAXblu,linestyle='--',alpha=0.5)
    #add text for average
    plt.title("PsyArXiv uploads per day for " + monthname + " " + str(year),color=PAXblu)
    plt.xlabel("Day",color=PAXblu)
    plt.ylabel("Uploads (horizontal line shows average)",color=PAXblu)
    #plt.ylim([0,100])
    # remove x labels
    plt.xticks([])
    plt.savefig(savename,dpi=300,bbox_inches='tight')
  
# count preprints in a given year
def count_preprints(api_token,year):
    """
    Retrieves the number of preprints created within timewindow from the OSF API.

    Parameters:
        api_token (str): Personal access token for OSF API authentication.

    Returns:
        int: Total count of preprints created in 2024.
    """
    # Base URL for the OSF API
    base_url = "https://api.osf.io/v2/preprints/"
    
    # Query parameters for filtering preprints created

    # First day of the year
    start_date = datetime(year, 1, 1, 0, 0, 0).date()
    # Last day of the year
    end_date = datetime(year, 12, 31, 23, 59, 59).date()

    params = {
        "filter[date_created][gte]": start_date,
        "filter[date_created][lt]": end_date,
        "filter[provider]": "psyarxiv",
    }
    
    # Headers with the API token for authentication
    headers = {
        "Authorization": f"Bearer {api_token}",
    }
    
    # Make the API request
    response = requests.get(base_url, headers=headers, params=params)
    
    # Check if the request was successful
    if response.status_code == 200:
        # Parse the JSON response and extract the total count
        data = response.json()
        total_preprints = data.get("links", {}).get("meta", {}).get("total")
        if total_preprints is not None:
            return total_preprints
        else:
            print("Total count not found in response.")
            return None
    else:
        # Print an error message if the request failed
        print(f"Error: {response.status_code} - {response.text}")
        return None

def mastodon_post(tootline,mediafile=None,alttxt=None):
    '''post to mastodon'''
    
    user = 'PAXscraper@mastodon.social'
    (username, domain) = user.split('@')

    #   Set up Mastodon
    mastodon = Mastodon(
        access_token = 'token_PAX.secret',
        api_base_url = 'https://mastodon.social/'
    )

    # ------------------------------------post to mastodon
    if mediafile==None:
        print("Tooting textonly")
        mastodon.status_post(tootline)
    else:
        print("Tooting text + media")
        mime_type = 'image/png'  # Manually specify the MIME type
        media = mastodon.media_post(mediafile, mime_type=mime_type, description=alttxt)
        time.sleep(3) #wait a few seconds for the media to upload
        mastodon.status_post(tootline, media_ids=media)
        
def report(args):
    '''just a demo of how to report passed arguments'''
    # Access parsed arguments here
    print(f"TYPE: {args.TYPE}")
    print(f"WINDOW: {args.WINDOW}")

def get_PAXtoken():
    '''from file load api token for PsyArXiv'''
    api_file = "token_PAXts.secret" 
    with open(api_file, 'r') as file:
        api_token = file.read().strip() 

    return api_token

if __name__ == '__main__':

    # --------------------- Initialize the parser
    parser = argparse.ArgumentParser(description="A script to process preprints data.")

    # Add arguments
    parser.add_argument('--TYPE', type=str, default="week",help="What are we doing today? Week review, or month, or year.")
    parser.add_argument('--WINDOW', type=int, default=7, help="How many days in the past to look.")

    # Parse arguments
    args = parser.parse_args()

    #report
    report(args)


    # -----------------------params & set up
    TYPE = args.TYPE
    WINDOW = args.WINDOW #days = 1 week + today
    SAVEENV = False #toggle, export conda environment

    print("identifying host machine")
    #test which machine we are on and set working directory
    if 'tom' in socket.gethostname():
        os.chdir('/home/tom/t.stafford@sheffield.ac.uk/A_UNIVERSITY/toys/pax')
    else:
        print("Not sure whose machine we are on. Maybe the script will run anyway...")
    print("We are in :" + os.getcwd())
    
    #get today's date
    now = datetime.now() #.strftime('%Y-%m-%d')
    print("Now is " + str(now))

    #get day from today
    today = now.date()


    # -----------------------do different things
    if TYPE == 'week':
        do_week_review(today,WINDOW)
    elif TYPE == 'month':
        do_month_review(today)
    else:
        print("given TYPE argument not recognised")

    if False:
        start_date=today.replace(day = 1)
        end_date = today
        do_period_review(start_date,end_date)

    if False:
        #COUNT UPLOADS FOR EACH MONTH, MAKE & SAVE GRAPH
        df = pd.DataFrame()

        for year in range(2015,2025):
            for month in range(1,13):
                if month != 12:
                    today = today.replace(year = year, month = month)
                else:
                    today = today.replace(year = year+1, month = 1)

                savename='archive/'+str(year)+'-'+str(month).zfill(2)+'.png' 
                print(savename)
                count = do_month_review(today,savename)
                df = df.append({'year':year,'month':month,'count':count},ignore_index=True)


        df.to_csv('archive/archive.csv')

    if False:
        #use count_preprints function to get count for yeach year
        # this is with 3% off the by month method (sometimes higher, sometimes lower!)
        

        # get OSF API personal access token from file
        api_token = get_PAXtoken()

        #2024 - 15548 uploads, long method = 15555
        df = pd.DataFrame()
        
        for year in range(2015,2026):
    
            preprints_count = count_preprints(api_token,year)
            if preprints_count is not None:
                print("Number of preprints created in " + str(year) + f": {preprints_count}") 

            df = df.append({'year':year,'count':preprints_count},ignore_index=True)

        df.to_csv('data/count_archive.csv')


    # ----------------------- quit
    #export environment in which this was last run
    if SAVEENV:
        os.system('conda env export > webscrape.yml') 
    
